%        File: what_is_PRML.tex
%     Created: 周四 三月 01 09:00 上午 2012 中国标准时间
% Last Change: 周四 三月 01 09:00 上午 2012 中国标准时间
%
\documentclass[11pt,a4paper]{article}
\usepackage{CJK}
\usepackage{amsmath,graphicx,amsthm,amssymb}
\theoremstyle{remark}
\newtheorem{cdefi}{\bf definition}
\newtheorem{law}{\bf law}
\title{PRML\,Probability}
\author{@seaslee}
\begin{document}
\maketitle
\begin{CJK*}{GBK}{song}
在模式识别领域中，我们遇到的一个很关键的问题就是不确定性。概率论为我们解决这种不确定性提供了一个系统的框架。在得到了相关变量的概率信息后，我们需要用决策论的相关知识做出最优的判断。也就是说，我们将模式识别的过程分为了两个阶段，第一个阶段就是推理(inference),得到相关的后验概率；第二阶段使用决策论知识做出最优的判断。下面就是概率论和决策论要用到的知识。
\section{概率}
概率论就是研究不确定现象的数学。概率是对随机事件发生可能性的度量。对于概率的基础知识我们在大学本科阶段都有学习，这里简单的回顾一下相关的知识
\begin{cdefi}
  如果一个函数$p:S\to\mathbb{R}, A\to p(A)$指定给每一个事件空间$S$中的事件$A$一个实数$P(A)$,满足以下三条公理:
  \[0<=P(A)<=1;
    \]
  \[P(S)=1;
    \]
  \[P(A\cup B)=P(A)+P(B),if P(A\cap B)=0.
    \]
    那么函数$P$叫做概率函数，相应的$P(A)$就是事件$A$的概率。
\end{cdefi}
对我们来说，概率有两个非常重要的定理，加法定律和乘法定律。对我们以后的概率分析有非常重要的作用。
\begin{law}
  $$p(X)=\sum_Yp(X,Y);$$
\end{law}
\begin{law}
  $$p(X,Y)=p(Y|X)p(X);$$
\end{law}
在下面的分析中，必须要好好地运用这两个定理。
\section{贝叶斯定理}
\begin{cdefi}
  \[
    p(Y|X)=\frac{p(X|Y)p(Y)}{p(X)}
    \]
\end{cdefi}
其中$p(Y|X)$表示在X发生情况下，Y发生的概率，是一个条件概率。

贝叶斯主义者和频率主义者对这个定理有不同的解释，在贝叶斯主义者看来，概率代表的是信任度，贝叶斯定理解释了在一个命题中，在考虑了证据后对信任度的影响；而频率主义者看来，概率代表了事件发生的个数与事件空间总的数目的比值，贝叶斯定理描述了特定事件概率值之间的关系。

在贝叶斯解释中，

$p(Y)$表示的是先验概率(prior)，$Y$初始的信任度;

$p(Y|X)$表示的是后验概率(poster)，考虑了$X$后的信任度;

$\frac{p(X|Y)p(Y)}{P(X)}$表示$X$对$Y$的支持;其中，p(X|Y)称为似然函数（likelihood).

上面的贝叶斯概率解释在PRML里面，从始至终都在使用，所以这个理论的基础知识还是必须知道的。由于在实际的使用中，X的概率通常是一定的，我们可以省略掉，所以贝叶斯定理也可以表示为：

$poster \propto likelihood \times prior $

这在PRML概率计算后验概率时，非常常用，对于我们理解很有帮助。

\section{概率分布}
了解了概率的基本知识以后，了解常见的概率分布是必要的。具体的描述，可以参考各种概率论书籍或是PRML的第二章。我们在这里只了解一下最常用的高斯分布（正态分布）。

$$\mathcal N(x|\mu,\sigma)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$

其中，$\mu$是均值，$\sigma$是标准差(其中$\sigma^2$是方差)。
在这里，下面是高斯分布的几何表示：($\mu=1,\sigma=1$)。

\includegraphics[width=12cm,height=8cm]{gaussian.eps}
\section{总结}
概率论在机器学习和模式识别中有基础性的作用，现在的机器学习算法基本都构建在概率论的基础上的。这是因为在这些研究中，我们常常遇到的一个难题就是应对不确定性。面对这些不确定性，概率论为我们提供了一个完整的框架。比如：在图像标注问题中，一张X照，让分类器做正常还是不正常的分类，我们经常做的就是利用概率知识，来推断后验概率模型，然后在用它来做最优的判断。

现在的知识储备还是不行，大学里面学的都是经典的概率论，关于贝叶斯观点的概率根本就没有介绍。但是在PRML这本书里面将贝叶斯作为了一个主轴，有时间必须好好看看《概率论沉思录》。

\section{决策论}
决策论是一个交叉的学科，他教人们如何决策，来达到最优决策效果。在这里我们只关注和机器学习相关的一点知识。在这里我们接触到的一个非常重要的概念就是：loss function。这在后面我们的线性分类和回归模型中都有用到。

loss function用来测量我们在做出某一个决策时发生的损失。例如：一个输入变量$x$,它输入$C_k$类，我们的决策却将其分在了$C_j$类，这时我们就说发生了损失，不妨用$L_kj$来表示。设$x,C_k$的联合概率用$p(x,C_k)$来表示，则loss function的期望就是：
$$\mathbb E(L)=\sum_k\sum_j\int_{R_j}L_{kj}p(x,C_k)dx$$
上面就是分类的loss function，我们的目的就是最小化这个函数。

在分类问题中，我们一般采用三种方式解决，

1)判别函数，直接由训练集合的好一个函数$f(x)$，用来对新的输入做判断。

2)生成模型，由训练集得到$p(x|C_j),p(C_j)$,然后利用贝叶斯定理得到后验概率$p(C_j|x)$,在根据后验做最优判断。

3)判别模型，直接由训练集合得到后验概率$p(C_j|x)$，然后用它来做最优判断。

可以看出，后验概率$p(C_j|x)$，非常重要，在实际的判别中起到重要作用。

最大似然估计：
最大似然估计会寻找关于的最可能的值（即在所有可能的取值中，寻找一个值使这个采样的“可能性”最大化）。等价于优化最小平方和误差函数。实际应用中一般会取似然函数的对数作为求最大值的函数，这样求出的最大值和直接求最大值得到的结果是相同的。似然函数的最大值不一定唯一，也不一定存在。

$argmax_Yp(X|Y)$

最大后验估计：
最大后验估计是根据经验数据获得对难以观察的量的点估计。它与最大似然估计中的经典方法有密切关系，但是它使用了一个增大的优化目标，这种方法将被估计量的先验分布融合到其中。所以最大后验估计可以看作是规则化（regularization）的最大似然估计。

$argmax_Yp(X|Y)p(Y)$

最大后验估计可以用以下几种方法计算：
1.解析方法，当后验分布的模能够用 closed form 方式表示的时候用这种方法。当使用conjugate prior 的时候就是这种情况。

2.通过如共扼积分法或者牛顿法这样的数值优化方法进行，这通常需要一阶或者导数，导数需要通过解析或者数值方法得到。

3.通过期望最大化算法的修改实现，这种方法不需要后验密度的导数。

尽管使用了先验知识，但是MAP通常不被认为是一种贝叶斯估计，因为它实际还是一种点估计，而贝叶斯使用这些分布来总结数据、得到推论。Bayesian 方法试图算出后验均值或者中值以及posterior interval，而不是后验模。MAP相当于在平方和误差函数的基础上，增加一个正则化项。

\clearpage
\end{CJK*}
\end{document}

